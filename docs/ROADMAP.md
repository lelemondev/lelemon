# Lelemon Roadmap

## Completed

### Phase 1: Core MVP
- [x] Go backend structure (Clean Architecture)
- [x] Domain entities (Project, Trace, Span)
- [x] SQLite store implementation
- [x] Ingest service + handler
- [x] API Key auth middleware
- [x] Health endpoint
- [x] Cost calculation (25+ models)

### Phase 2: Full API
- [x] Trace service (CRUD)
- [x] Analytics service (stats, usage time series)
- [x] Project service (CRUD + rotate key)
- [x] Rate limiting (100 req/min)
- [x] All handlers

### Phase 3: Dashboard Auth
- [x] JWT auth (register/login)
- [x] Google OAuth integration
- [x] Session middleware
- [x] Dashboard routes

### Phase 4: PostgreSQL
- [x] PostgreSQL store implementation
- [x] Connection pooling (pgx/v5, 5-25 connections)
- [x] Native JSONB support for metadata

### Phase 5: ClickHouse
- [x] ClickHouse store implementation
- [x] ReplacingMergeTree for Users/Projects/Traces (updates)
- [x] MergeTree for Spans (append-only, high-volume)
- [x] Optimized analytics queries (toStartOfHour, toDate, etc.)
- [x] Batch inserts via PrepareBatch
- [x] Full test coverage

### Phase 6: Production Ready
- [x] Structured logging (slog) with request IDs
- [x] Graceful shutdown with 30s timeout
- [x] Docker compose examples (SQLite, PostgreSQL, ClickHouse)
- [x] Improved health checks (/health, /health/live, /health/ready)

### TypeScript SDK (`@lelemondev/sdk`)
- [x] Core SDK: `init()`, `observe()`, `flush()`, `captureSpan()`
- [x] Batch transport (10 spans or 1 second)
- [x] Auto-detect OpenAI/Anthropic/Gemini/Bedrock/OpenRouter formats
- [x] Zero dependencies, tree-shakeable
- [x] Compatible with Go backend `/api/v1/ingest` endpoint

---

## In Progress

### Phase 7: Hierarchical Tracing & Visualization

#### Problema Actual
Cada llamada LLM crea un trace independiente. No hay agrupaciÃ³n de pasos de un agente bajo un trace padre.

```
ACTUAL:
  Trace 1 (LLM call) â†’ Span 1
  Trace 2 (LLM call) â†’ Span 1
  Trace 3 (LLM call) â†’ Span 1

DESEADO:
  Trace 1 (Agent workflow)
    â”œâ”€â”€ Span 1 (Retrieval)
    â”œâ”€â”€ Span 2 (LLM call) â†’ thinking, tool_use
    â”‚   â””â”€â”€ Span 3 (Tool execution)
    â””â”€â”€ Span 4 (LLM call) â†’ final response
```

#### 7.1 Enriquecer Captura del SDK
**Sin cambiar la API pÃºblica.**

- [ ] Extraer `stopReason` / `finishReason`
  - Anthropic: `response.stop_reason` ('end_turn', 'tool_use', 'max_tokens')
  - OpenAI: `choice.finish_reason` ('stop', 'tool_calls', 'length')
- [ ] Extraer tokens de cache (Anthropic)
  - `usage.cache_read_input_tokens`
  - `usage.cache_creation_input_tokens`
- [ ] Extraer reasoning tokens (OpenAI o1)
  - `usage.completion_tokens_details.reasoning_tokens`
- [ ] Extraer `thinking` blocks (Claude extended thinking)
  - `content.filter(b => b.type === 'thinking')`
- [ ] Medir `firstTokenMs` en streaming
- [ ] Actualizar schema del backend con nuevos campos

**Nuevos campos en Span:**
```typescript
{
  stopReason?: string;       // 'end_turn' | 'tool_use' | 'stop' | 'max_tokens'
  cacheReadTokens?: number;
  cacheWriteTokens?: number;
  reasoningTokens?: number;
  firstTokenMs?: number;
  thinking?: string;
}
```

#### 7.2 Contexto de Trace (AsyncLocalStorage)
**Nueva API opcional, la API simple sigue funcionando igual.**

```typescript
import { observe, withTrace } from '@lelemondev/sdk';

// API SIMPLE (sin cambios, sigue funcionando)
const client = observe(new Anthropic());
await client.messages.create({...}); // Trace independiente

// NUEVA API: Agrupar bajo un trace padre
await withTrace({ name: 'sales-agent', input: userMessage }, async () => {
  await client.messages.create({...}); // Span 1 bajo el trace
  await client.messages.create({...}); // Span 2 bajo el trace
  await client.messages.create({...}); // Span 3 bajo el trace
});
```

**ImplementaciÃ³n:**
- [ ] `AsyncLocalStorage` para contexto implÃ­cito
- [ ] `withTrace(options, fn)` - crea trace padre y ejecuta fn
- [ ] Modificar `captureTrace` para usar contexto
- [ ] Propagar `traceId` y `parentSpanId` automÃ¡ticamente
- [ ] Crear span raÃ­z tipo `agent` automÃ¡ticamente

#### 7.3 Spans Manuales para Tools y Retrieval
**Para operaciones no-LLM.**

```typescript
import { captureSpan } from '@lelemondev/sdk';

// Dentro de withTrace
const t0 = Date.now();
const docs = await pinecone.query({ vector, topK: 5 });
captureSpan({
  type: 'retrieval',
  name: 'pinecone-search',
  input: { query, topK: 5 },
  output: { count: docs.length },
  durationMs: Date.now() - t0,
});
```

**Tipos de span soportados:**
| Tipo | Icono | Uso |
|------|-------|-----|
| `agent` | ğŸ¤– | Trace raÃ­z / workflow |
| `llm` | ğŸ”· | Llamada a modelo LLM |
| `tool` | ğŸ”§ | EjecuciÃ³n de herramienta |
| `retrieval` | ğŸ” | BÃºsqueda vectorial / RAG |
| `embedding` | ğŸ“Š | GeneraciÃ³n de embeddings |
| `rerank` | ğŸ¯ | Reranking de documentos |
| `guardrail` | ğŸ›¡ï¸ | ValidaciÃ³n de contenido |
| `custom` | ğŸ“‹ | Cualquier operaciÃ³n |

#### 7.4 UI de VisualizaciÃ³n (3 Columnas)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACES                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ COL 1       â”‚ â”‚ COL 2                     â”‚ â”‚ COL 3                 â”‚ â”‚
â”‚ â”‚ TRACE LIST  â”‚ â”‚ TRACE TIMELINE            â”‚ â”‚ SPAN DETAIL           â”‚ â”‚
â”‚ â”‚             â”‚ â”‚                           â”‚ â”‚                       â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â–¼ ğŸ¤– sales-agent          â”‚ â”‚ ğŸ”· GENERATION         â”‚ â”‚
â”‚ â”‚ â”‚sales-ag â”‚ â”‚ â”‚   â”œâ”€ ğŸ” vector-search     â”‚ â”‚ claude-sonnet-4       â”‚ â”‚
â”‚ â”‚ â”‚8.5s $0.02â”‚ â”‚ â”‚   â”œâ”€ ğŸ”· intent-class     â”‚ â”‚ 2.8s | $0.0089        â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚   â”œâ”€ ğŸ”· agent-response â—€â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚   â”‚   â””â”€ ğŸ”§ schedule_demo â”‚ â”‚ ğŸ’­ THINKING           â”‚ â”‚
â”‚ â”‚ â”‚rag-quer â”‚ â”‚ â”‚   â””â”€ ğŸ”· final-response    â”‚ â”‚ "El usuario quiere    â”‚ â”‚
â”‚ â”‚ â”‚2.1s $0.01â”‚ â”‚ â”‚                           â”‚ â”‚  agendar para..."     â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ INPUT: "hola quisiera..." â”‚ â”‚                       â”‚ â”‚
â”‚ â”‚             â”‚ â”‚ OUTPUT: "Â¡Listo Antonio!" â”‚ â”‚ ğŸ”§ TOOL USE           â”‚ â”‚
â”‚ â”‚             â”‚ â”‚                           â”‚ â”‚ schedule_demo({...})  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- [ ] Layout 3 columnas responsive
- [ ] TraceList component (col 1)
- [ ] TraceTimeline component (col 2) - Ã¡rbol jerÃ¡rquico
- [ ] SpanDetail component (col 3)
- [ ] Renderizar `thinking` blocks
- [ ] Renderizar `tool_use` con input/output
- [ ] Timeline visual (barras de duraciÃ³n)
- [ ] Indicadores de error/status
- [ ] Mobile: tabs o drawer

---

## Estructura de Datos Objetivo

### Trace
```typescript
interface Trace {
  id: string;
  projectId: string;
  name?: string;
  sessionId?: string;
  userId?: string;

  input?: unknown;   // Input inicial del workflow
  output?: unknown;  // Output final del workflow

  status: 'active' | 'completed' | 'error';
  errorMessage?: string;

  metadata?: Record<string, unknown>;
  tags?: string[];

  // MÃ©tricas agregadas (calculadas de spans)
  metrics: {
    totalSpans: number;
    totalLLMCalls: number;
    totalToolCalls: number;
    totalTokens: number;
    totalInputTokens: number;
    totalOutputTokens: number;
    totalCostUsd: number;
    totalDurationMs: number;
  };

  createdAt: Date;
  endedAt?: Date;
}
```

### Span
```typescript
interface Span {
  id: string;
  traceId: string;
  parentSpanId?: string;  // Para jerarquÃ­a

  type: 'agent' | 'llm' | 'tool' | 'retrieval' | 'embedding' | 'guardrail' | 'custom';
  name: string;

  // LLM especÃ­fico
  model?: string;
  provider?: string;

  input?: unknown;
  output?: unknown;

  // Tokens
  inputTokens?: number;
  outputTokens?: number;
  cacheReadTokens?: number;
  cacheWriteTokens?: number;
  reasoningTokens?: number;

  // Timing
  durationMs?: number;
  firstTokenMs?: number;

  // Status
  status: 'pending' | 'success' | 'error';
  stopReason?: string;
  errorMessage?: string;

  // Extras
  thinking?: string;
  toolCallId?: string;

  costUsd?: number;
  metadata?: Record<string, unknown>;

  startedAt: Date;
  endedAt?: Date;
}
```

---

## Ejemplos de Uso del SDK

### Caso 1: API Simple (sin cambios)
```typescript
import { init, observe } from '@lelemondev/sdk';
import Anthropic from '@anthropic-ai/sdk';

init({ apiKey: process.env.LELEMON_API_KEY });

const client = observe(new Anthropic(), {
  sessionId: 'conv_123',
  userId: 'user_456',
});

// Cada llamada = 1 trace + 1 span (igual que ahora)
await client.messages.create({
  model: 'claude-sonnet-4-20250514',
  messages: [{ role: 'user', content: 'Hola' }],
});
```

### Caso 2: Agente con Trace Padre
```typescript
import { init, observe, withTrace } from '@lelemondev/sdk';

init({ apiKey: process.env.LELEMON_API_KEY });
const client = observe(new Anthropic());

async function handleUserMessage(userMessage: string) {
  return withTrace({
    name: 'sales-agent',
    input: { message: userMessage },
    metadata: { channel: 'whatsapp' },
  }, async () => {
    // LLM 1: Clasificar intenciÃ³n
    await client.messages.create({ model: 'claude-3-5-haiku-20241022', ... });

    // LLM 2: Generar respuesta con tools
    const response = await client.messages.create({
      model: 'claude-sonnet-4-20250514',
      tools: [...],
      ...
    });

    // Si hay tool_use, ejecutar y continuar
    if (response.stop_reason === 'tool_use') {
      // Ejecutar tools (captureSpan automÃ¡tico o manual)
      // LLM 3: Respuesta final
      await client.messages.create({ ... });
    }

    return response;
  });
}
```

### Caso 3: RAG con Retrieval Manual
```typescript
async function ragQuery(question: string) {
  return withTrace({ name: 'rag-query', input: question }, async () => {
    // 1. Embedding (manual)
    const t0 = Date.now();
    const embedding = await openai.embeddings.create({ input: question, model: 'text-embedding-3-large' });
    captureSpan({
      type: 'embedding',
      name: 'query-embedding',
      input: { text: question },
      output: { dimensions: 3072 },
      durationMs: Date.now() - t0,
    });

    // 2. Retrieval (manual)
    const t1 = Date.now();
    const docs = await pinecone.query({ vector: embedding, topK: 5 });
    captureSpan({
      type: 'retrieval',
      name: 'pinecone-search',
      input: { topK: 5 },
      output: { count: docs.length },
      durationMs: Date.now() - t1,
    });

    // 3. LLM (automÃ¡tico)
    return client.messages.create({
      system: buildPromptWithDocs(docs),
      messages: [{ role: 'user', content: question }],
    });
  });
}
```

### Caso 4: Agente con Guardrails
```typescript
async function safeChat(userMessage: string) {
  return withTrace({ name: 'safe-chat' }, async () => {
    // 1. Input guardrail
    const t0 = Date.now();
    const inputCheck = await checkContent(userMessage);
    captureSpan({
      type: 'guardrail',
      name: 'input-safety',
      input: { content: userMessage },
      output: { passed: inputCheck.safe, violations: inputCheck.violations },
      status: inputCheck.safe ? 'success' : 'error',
      durationMs: Date.now() - t0,
    });

    if (!inputCheck.safe) throw new Error('Input blocked');

    // 2. LLM
    const response = await client.messages.create({ ... });

    // 3. Output guardrail
    const t1 = Date.now();
    const outputCheck = await checkContent(response.content);
    captureSpan({
      type: 'guardrail',
      name: 'output-safety',
      input: { content: response.content },
      output: { passed: outputCheck.safe },
      durationMs: Date.now() - t1,
    });

    return response;
  });
}
```

---

## Backlog

### Vercel AI SDK Integration
- [ ] `withLelemon(model)` wrapper
- [ ] Automatic token extraction
- [ ] Streaming support

### Framework Integrations
- [ ] LangChain callback handler
- [ ] LlamaIndex callback
- [ ] Haystack integration

### Future Considerations

#### GDPR & Compliance
- Self-hosted deployment covers most needs
- Formal compliance documentation (if needed for enterprise)

#### Pricing Tiers (Cloud)
- Free: Self-hosted
- Pro: ~$50/month (hosted, 1M spans)
- Enterprise: Custom pricing

#### Competitive Position
| Feature | Lelemon | Langfuse | Arize |
|---------|---------|----------|-------|
| Self-hosted | âœ… | âœ… | âŒ |
| RAM usage | ~50MB | ~500MB | N/A |
| Language | Go | TypeScript | Python |
| Price | Free | Free/$59 | $800+ |
| Hierarchical traces | âœ… (Phase 7) | âœ… | âœ… |
| Extended thinking | âœ… (Phase 7) | âŒ | âŒ |
